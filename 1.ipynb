{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-10-16T15:21:17.215447Z"
    }
   },
   "source": [
    "# === Импорты ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from datasets import Dataset\n",
    "\n",
    "# === Загрузка данных ===\n",
    "train_df = pd.read_csv(\"data/preprocessed/clean_train.csv\", sep=\";\")\n",
    "test_df = pd.read_csv(\"data/preprocessed/clean_test.csv\", sep=\";\")\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# === Токенизация ===\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"message\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "\n",
    "train_enc = train_dataset.map(tokenize_function, batched=True)\n",
    "test_enc = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_enc.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"is_toxic\"])\n",
    "test_enc.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"is_toxic\"])\n",
    "\n",
    "# === Модель и устройство ===\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# === DataLoader и оптимизатор ===\n",
    "train_loader = DataLoader(train_enc, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_enc, batch_size=8)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# === Обучение ===\n",
    "epochs = 3\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\")\n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != \"is_toxic\"}\n",
    "        labels = batch[\"is_toxic\"].to(device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Средняя потеря за эпоху {epoch + 1}: {avg_loss:.4f}\")\n",
    "\n",
    "# === Оценка ===\n",
    "model.eval()\n",
    "preds, labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Оценка\"):\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != \"is_toxic\"}\n",
    "        y_true = batch[\"is_toxic\"].to(device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        y_pred = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "        preds.extend(y_pred.cpu().numpy())\n",
    "        labels.extend(y_true.cpu().numpy())\n",
    "\n",
    "print(\"\\n=== Отчёт по метрикам ===\")\n",
    "print(classification_report(labels, preds, digits=3))\n",
    "print(\"F1-score:\", round(f1_score(labels, preds), 3))\n",
    "\n",
    "# === Сохранение модели ===\n",
    "model.save_pretrained(\"models/roberta-toxic\")\n",
    "tokenizer.save_pretrained(\"models/roberta-toxic\")\n",
    "\n",
    "print(\"\\nМодель и токенизатор сохранены в 'models/roberta-toxic'\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1016 18:21:21.558000 95505 site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/12711 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f9988dfdcd09440082fa96b489f5d10c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/228 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa2a9637b888435482d7e0bb11b49c49"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1:  37%|███▋      | 581/1589 [03:28<06:45,  2.48it/s, loss=0.0406]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1e310c315bad30dc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
